import argparse
import datetime
import os
import copy
import re
import shlex  
import signal
import time
import warnings
import traceback
from tempfile import NamedTemporaryFile
import subprocess
import shutil
import sys
import multiprocessing as mp
from typing import Dict, List
from warnings import warn

import psutil
from stnd.run_with_monitor.gsheet_batch_updater import GSheetBatchUpdater
from stnd.run_with_monitor.job import Job, JobStatus, find_job_id_by_row_id, find_job_idx, get_slurm_job_status
from stnd.run_with_monitor.job_manager import JobManager
from stnd.utility.ai_center_cluster_specific import get_region, CLUSTER
from stnd.run_with_monitor.utility.local_processing_utils import process_exists

# Import job submission functionality
from stnd.run_with_monitor.job_submission import (
    get_all_slurm_jobs,
    get_pid_job_stats,
    get_slurm_jobs_stats,
    update_job_statuses_in_place,
    update_job_status,
    submit_job,
    run_jobs_flag,
    submitted_jobs,
    running_jobs,
    submitted_jobs_lock,
)

# Import job monitoring functionality
from stnd.run_with_monitor.job_monitor import (
    monitor_jobs_async,
    dump_into_gsheet_queue,
)

# Import CSV and configuration processing functionality
from stnd.run_with_monitor.config_processor import (
    process_csv_row,
    fetch_default_config_path,
    get_default_configs_folder,
    make_new_config,
    make_config_from_default_and_deltas,
    replace_placeholders,
    extract_from_csv_row_by_prefix,
)

# Import command generation functionality
from stnd.run_with_monitor.command_builder import (
    make_task_cmd,
    make_final_cmd_slurm,
    check_csv_column_names,
    validate_command_requirements,
)

# Import SLURM integration functionality
from stnd.run_with_monitor.slurm_utils import (
    fill_sbatch_script,
    make_slurm_args_dict,
    DEFAULT_SLURM_ARGS_DICT,
)

# local modules
sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))
from stnd.run_with_monitor.utility.utils import (
    DEFAULT_ENV_NAME,
    NEW_SHELL_INIT_COMMAND,
    make_autogenerated_config_name,
    read_yaml,
    save_as_yaml,
    update_dict_by_nested_key,
    get_project_root_path,
    decode_strings_in_dict,
    read_csv_as_dict,
    normalize_path,
    check_duplicates,
    expand_csv,
    retrier_factory,
    is_number,
)
from stnd.run_with_monitor.utility.configs import make_csv_config
from stnd.run_with_monitor.utility.constants import (
    AUTOGEN_PREFIX,
    NESTED_CONFIG_KEY_SEPARATOR,
    STATUS_CSV_COLUMN,
    SUBMITTED_STATUS,
    WHETHER_TO_RUN_COLUMN,
)
from stnd.run_with_monitor.utility.logger import (
    DELTA_PREFIX,
    SLURM_PREFIX,
    PREFIX_SEPARATOR,
    PLACEHOLDERS_FOR_DEFAULT,
    make_logger,
    make_gdrive_client,
    sync_local_file_with_gdrive,
    log_csv_for_concurrent,
    fetch_csv,
    try_to_upload_csv,
    make_delta_column_name,
    HTTP_PREFIX,
    GspreadClient,
)

# Arnas' changes
DELTA_AFFECTS_ONLY_FIXED_PARAMS = True
EMPTY_VALUE_MEANS_NO_CHANGE = True

##

FILES_URL = "https://drive.google.com/file"

USE_SRUN = False

OPEN_SOCKET = True

PATH_TO_DEFAULT_CONFIG_COLUMN = "path_to_default_config"
MAIN_PATH_COLUMN = "path_to_main"
DEV_NULL = "/dev/null"
EMPTY_STRING = "EMPTY_STRING"
EXPANDED_CSV_PREFIX = "expanded_"
CURRENT_ROW_PLACEHOLDER = "__ROW__"
CURRENT_WORKSHEET_PLACEHOLDER = "__WORKSHEET__"
MAX_PROCESSES = 16

# Monitor-related constants
MONITOR_STATUS_COLUMN = "status_monitor"
MONITOR_EXIT_CODE_COLUMN = "exit_code_monitor"
MONITOR_JOB_ID_COLUMN = "slurm_job_id_monitor"
MONITOR_LAST_UPDATE_COLUMN = "last_update_monitor"

# use_shared_memory = sys.version_info >= (3, 8, 3)
# if use_shared_memory:
#     pass


def parse_args():
    parser = argparse.ArgumentParser(description="Train and/or validate models.")
    parser.add_argument("--csv_path", type=str, required=True, help="path to csv file")
    parser.add_argument(
        "--conda_env",
        type=str,
        required=False,
        default=DEFAULT_ENV_NAME,
        help="conda environment name",
    )
    parser.add_argument(
        "--run_locally", action="store_true", help="whether to run this script locally"
    )
    parser.add_argument(
        "--log_file_path",
        type=str,
        required=False,
        default=get_default_log_file_path(),
        help="default path for the log file",
    )
    parser.add_argument(
        "--expand",
        action="store_true",
        help="whether to first expand input csv by cartesian product of options",
    )

    parser.add_argument(
        "--use_socket",
        action="store_true",
        help="whether to use a socket for communication with the server",
    )

    parser.add_argument(
        "--disable_local_loging",
        action="store_true",
        help="whether to disable logging to wandb and g drive for local runs",
    )

    parser.add_argument(
        "--max_concurrent_jobs",
        type=int,
        required=False,
        default=-1,
        help="maximum number of concurrent jobs to run",
    )
    return parser.parse_args()

def get_default_log_file_path():
    return os.path.join(get_project_root_path(), "tmp", "tmp_log_for_run_from_csv.out")


def main_with_monitoring(
    make_final_cmd=None, allowed_prefixes=(SLURM_PREFIX, DELTA_PREFIX, HTTP_PREFIX)
):
    # Determine to which region we belong
    cluster_region = get_region()
    if cluster_region == CLUSTER.UNKNOWN:
        print("Unknown cluster region. Using partitions are specified")
    else:
        print(f"We're on cluster region: {cluster_region}, using only the corresponding partitions")

    if make_final_cmd is None:
        make_final_cmd = make_final_cmd_slurm

    args = parse_args()
    max_conc_jobs = -1 if "max_concurrent_jobs" not in args else args.max_concurrent_jobs
    use_socket = args.use_socket if args.use_socket is not None else False
    disable_local_loging = args.disable_local_loging if args.disable_local_loging is not None else False

    logger = make_logger()

    if use_socket:
        logger.log("User requested to use socket.")

    job_manager = JobManager(
        local_run=args.run_locally if args.run_locally is not None else False,
        open_socket=use_socket,
        logger=logger,
    )

    # Make sure server ip and port are set
    if job_manager.open_socket:
        logger.log("Trying to get server ip and port")
        while job_manager.server_ip is None or job_manager.server_port is None:
            time.sleep(1)

    csv_path_or_url = args.csv_path
    logger.log(f"Fetching csv from: {csv_path_or_url}")
    csv_path, spreadsheet_url, worksheet_name, gspread_client = fetch_csv(csv_path_or_url, logger)

    if args.expand:
        expand_gsheet(csv_path, spreadsheet_url, worksheet_name, gspread_client)

    inputs_csv = read_csv_as_dict(csv_path)

    with mp.Manager() as shared_memory_manager:
        lock = shared_memory_manager.Lock()
        current_step = shared_memory_manager.Value("int", 0)
        shared_rows_to_run = shared_memory_manager.list()
        shared_csv_updates = shared_memory_manager.list()
        shared_default_config_paths = shared_memory_manager.dict()
        shared_row_numbers = shared_memory_manager.list()
        shared_jobs_dict = shared_memory_manager.dict()

        # First process all rows to prepare commands
        starmap_args_for_row_processing = [
            (
                make_final_cmd,
                csv_row,
                row_number,
                csv_path,
                args.conda_env,
                args.run_locally,
                args.log_file_path,
                spreadsheet_url,
                worksheet_name,
                logger,
                lock,
                shared_rows_to_run,
                shared_default_config_paths,
                shared_csv_updates,
                shared_row_numbers,
                current_step,
                len(inputs_csv),
                job_manager.server_ip,
                job_manager.server_port,
                disable_local_loging,
                cluster_region,
            )
            for row_number, csv_row in inputs_csv.items()
        ]

        if len(starmap_args_for_row_processing):
            pool_size = get_pool_size(len(starmap_args_for_row_processing))
            with mp.Pool(pool_size) as pool:
                # Process all rows first to prepare commands
                pool.starmap(process_csv_row, starmap_args_for_row_processing)

                if len(shared_rows_to_run):
                    assert 2 * len(shared_rows_to_run) == len(shared_csv_updates)
                    concurrent_log_func = retrier_factory()(log_csv_for_concurrent)
                    concurrent_log_func(csv_path, shared_csv_updates, use_socket=True)
                    os.makedirs(os.path.dirname(args.log_file_path), exist_ok=True)

                    # Prepare job submission arguments
                    job_submission_args = [
                        (
                            run_cmd,
                            args.log_file_path,
                            args.run_locally,
                            shared_jobs_dict,
                            row_id,
                            lock,
                            logger,
                            max_conc_jobs,
                        )
                        for run_cmd, row_id in zip(shared_rows_to_run, shared_row_numbers)
                    ]

                    # Submit jobs in batches and monitor them
                    monitor_jobs_async(
                        job_manager,
                        pool,
                        job_submission_args,
                        shared_jobs_dict,
                        args.run_locally,
                        logger,
                        spreadsheet_url,
                        worksheet_name,
                        shared_row_numbers,
                        csv_path,
                        gspread_client,
                        lock,
                        len(job_submission_args),
                        inputs_csv,
                        max_conc_jobs,
                    )

    # Clean up server
    if job_manager.open_socket:
        try:
            logger.log("Attempting to stop the server...")
            if job_manager.server_process and job_manager.server_process.is_alive():
                job_manager.server_process.terminate()
                job_manager.server_process.join(timeout=5)
                if job_manager.server_process.is_alive():
                    logger.log("Server didn't terminate gracefully, forcing...")
                    job_manager.server_process.kill()
            logger.log("Server stopped successfully")
        except Exception as e:
            logger.log(f"Error while stopping server: {str(e)}")

    logger.log("Job done, exiting.")
    os._exit(0)


def get_pool_size(iterable_len):
    return min(min(max(1, mp.cpu_count() - 1), iterable_len), MAX_PROCESSES)

def expand_gsheet(csv_path, spreadsheet_url, worksheet_name, gspread_client):
    expanded_csv_path = os.path.join(
        os.path.dirname(csv_path), EXPANDED_CSV_PREFIX + os.path.basename(csv_path)
    )
    expand_csv(csv_path, expanded_csv_path)
    csv_path = expanded_csv_path
    if worksheet_name is not None:
        worksheet_name = EXPANDED_CSV_PREFIX + worksheet_name

    try_to_upload_csv(csv_path, spreadsheet_url, worksheet_name, gspread_client)


if __name__ == "__main__":
    main_with_monitoring()