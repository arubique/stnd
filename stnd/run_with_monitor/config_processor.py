"""
CSV & Configuration Processing Module

This module handles:
- Individual CSV row processing
- Configuration file generation and management
- Configuration merging and delta application
- Default config path fetching and caching
- Placeholder substitution
"""

import os
import copy
import warnings
import traceback
import shutil
from tempfile import NamedTemporaryFile

from stnd.run_with_monitor.utility.ai_center_cluster_specific import CLUSTER
from stnd.run_with_monitor.utility.utils import (
    make_autogenerated_config_name,
    read_yaml,
    save_as_yaml,
    update_dict_by_nested_key,
    get_project_root_path,
    decode_strings_in_dict,
    normalize_path,
    check_duplicates,
    NEW_SHELL_INIT_COMMAND,
)
from stnd.run_with_monitor.utility.configs import make_csv_config
from stnd.run_with_monitor.utility.constants import (
    AUTOGEN_PREFIX,
    NESTED_CONFIG_KEY_SEPARATOR,
    STATUS_CSV_COLUMN,
    SUBMITTED_STATUS,
    WHETHER_TO_RUN_COLUMN,
)
from stnd.run_with_monitor.utility.logger import (
    DELTA_PREFIX,
    PREFIX_SEPARATOR,
    PLACEHOLDERS_FOR_DEFAULT,
    make_gdrive_client,
    sync_local_file_with_gdrive,
    make_delta_column_name,
)

# Import command building functionality
from stnd.run_with_monitor.command_builder import make_task_cmd

# Constants
FILES_URL = "https://drive.google.com/file"
PATH_TO_DEFAULT_CONFIG_COLUMN = "path_to_default_config"
MAIN_PATH_COLUMN = "path_to_main"
EMPTY_STRING = "EMPTY_STRING"
CURRENT_ROW_PLACEHOLDER = "__ROW__"
CURRENT_WORKSHEET_PLACEHOLDER = "__WORKSHEET__"

# Global flags from main module
DELTA_AFFECTS_ONLY_FIXED_PARAMS = True
EMPTY_VALUE_MEANS_NO_CHANGE = True


def process_csv_row(
    make_final_cmd,
    csv_row,
    row_number,
    input_csv_path,
    conda_env,
    run_locally,
    log_file_path,
    spreadsheet_url,
    worksheet_name,
    logger,
    lock,
    shared_rows_to_run,
    shared_default_config_paths,
    shared_csv_updates,
    shared_row_numbers,
    current_step,
    total_rows,
    server_ip,
    server_port,
    disable_local_loging,
    cluster_region,
):
    """Process a single CSV row and prepare it for job submission."""
    assert not spreadsheet_url or worksheet_name is not None, (
        "`worksheet_name` is None but this is not allowed when remote sheet is used;"
        "Make sure to pass the worksheet name using the `::` syntax in the --csv_path argument."
    )
    default_config_path = ""
    final_cmd = None

    whether_to_run = csv_row[WHETHER_TO_RUN_COLUMN]

    gpu_fits_cluster = True
    # check if the partition corresponds to login node
    if "slurm:partition" in csv_row:
        try:
            partition = csv_row["slurm:partition"]
        except Exception as e:
            logger.log(f"Skipping row {row_number} - 'slurm:partition' not found in csv_row")
            logger.log(f"Error: {e}")
            return
        # using galvani's gpus or ferranti's
        try:
            if "h100" in partition and cluster_region == CLUSTER.GALVANI:
                gpu_fits_cluster = False
                logger.log(
                    f"Skipping row {row_number} - H100 job not supported on Galvani cluster"
                )
                return
            if ("a100" in partition and cluster_region == CLUSTER.FERRANTI) or ("2080" in partition and cluster_region == CLUSTER.FERRANTI):
                gpu_fits_cluster = False
                logger.log(
                    f"Skipping row {row_number} - {partition} job not supported on Ferranti cluster"
                )
                return
        except Exception as e:
            logger.log(f"Skipping row {row_number} - 'slurm:partition' not found in csv_row")
            logger.log(f"Error: {e}")
            return
            
    try:
        if gpu_fits_cluster and whether_to_run.isnumeric() and int(whether_to_run) != 0:
            replace_placeholders(csv_row, CURRENT_ROW_PLACEHOLDER, str(row_number))
            replace_placeholders(csv_row, CURRENT_WORKSHEET_PLACEHOLDER, worksheet_name)

            default_config_path_or_url = csv_row[PATH_TO_DEFAULT_CONFIG_COLUMN]
            if not default_config_path_or_url in shared_default_config_paths:
                with lock:
                    try:
                        default_config_path = fetch_default_config_path(
                            default_config_path_or_url, logger
                        )
                    except Exception as e:
                        # log also row number etc
                        logger.log(
                            f"Failed to fetch default config path at {default_config_path_or_url}."
                            f"\nIt occurs in row {row_number} of {input_csv_path}."
                        )
                        raise e
                    shared_default_config_paths[default_config_path_or_url] = default_config_path
            else:
                default_config_path = shared_default_config_paths[default_config_path_or_url]
            if default_config_path is None:
                error_msg = f"Default config path at {default_config_path} is None. (occurs in row {row_number} of {input_csv_path})"
                logger.log(error_msg)
                raise ValueError(error_msg)
            if not os.path.exists(default_config_path):
                error_msg = f"Default config path at {default_config_path} does not exist (occurs in row {row_number} of {input_csv_path})"
                logger.log(error_msg)
                raise FileNotFoundError(error_msg)

            exp_dir = normalize_path(os.path.dirname(default_config_path))
            exp_name = os.path.basename(exp_dir)

            default_config = read_yaml(default_config_path)

            if disable_local_loging:
                if "logging" in default_config and isinstance(default_config["logging"], dict):
                    default_config["logging"]["use_wandb"] = False
                    default_config["logging"]["gdrive_storage_folder"] = None

            _, new_config_path = make_new_config(
                csv_row,
                row_number,
                input_csv_path,
                default_config,
                exp_dir,
                spreadsheet_url,
                worksheet_name,
                server_ip,
                server_port,
                run_locally,
            )

            cmd_as_string = make_task_cmd(
                new_config_path,
                conda_env,
                normalize_path(csv_row[MAIN_PATH_COLUMN]),
                csv_row,  # pass the row since might need to overwrite running command
            )

            log_folder = os.path.dirname(log_file_path)
            if not os.path.exists(log_folder):
                with lock:
                    os.makedirs(log_folder, exist_ok=True)

            if run_locally:
                final_cmd = "{}".format(cmd_as_string)
            else:
                final_cmd = make_final_cmd(csv_row, exp_name, log_file_path, cmd_as_string)

        if final_cmd is not None:
            shared_csv_updates.append((row_number, STATUS_CSV_COLUMN, SUBMITTED_STATUS))
            shared_csv_updates.append((row_number, WHETHER_TO_RUN_COLUMN, "0"))
            shared_rows_to_run.append(final_cmd)
            shared_row_numbers.append(row_number)

        with lock:
            current_step.value += 1
            logger.progress("Rows processing.", current_step.value, total_rows)
            
    except Exception as e:
        logger.log(f"Error processing row {row_number}: {str(e)}\nTraceback: {traceback.format_exc()}")
        raise  # Re-raise the exception after logging


def fetch_default_config_path(path, logger):
    """Fetch default configuration path, handling both local paths and Google Drive URLs."""
    if FILES_URL in path:
        gdrive_client = make_gdrive_client(logger)
        with NamedTemporaryFile("w+t", delete=False) as tmp_file:
            remote_file = sync_local_file_with_gdrive(
                gdrive_client, tmp_file.name, path, download=True, logger=logger
            )

            file_path = os.path.join(
                get_default_configs_folder(),
                remote_file["title"].split(".")[0],
                f"default_config.yaml",
            )

            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            shutil.move(tmp_file.name, file_path)

        return file_path

    else:
        try:
            return normalize_path(path)
        except Exception as e:
            logger.log(f"Couldn't fetch default config path at {path}!")


def get_default_configs_folder():
    """Get the default configs folder path."""
    return os.path.join(get_project_root_path(), "experiment_configs")


def make_new_config(
    csv_row,
    row_number,
    input_csv_path,
    default_config,
    exp_dir,
    spreadsheet_url,
    worksheet_name,
    server_ip,
    server_port,
    run_locally,
):
    """
    Create a new configuration by applying deltas to the default config.
    
    Assume that `fixed_params` are the ones being passed to the jobs. We will move them to the config file
    but will update them according to the `deltas` in the config file.
    """
    deltas = extract_from_csv_row_by_prefix(
        csv_row, DELTA_PREFIX + PREFIX_SEPARATOR, ignore_values=PLACEHOLDERS_FOR_DEFAULT
    )

    if DELTA_AFFECTS_ONLY_FIXED_PARAMS:
        """Make changes only in the 'fixed_params' subdict of the config"""
        assert (
            "fixed_params" in default_config
        ), "If DELTA_AFFECTS_ONLY_FIXED_PARAMS is True, then 'fixed_params' must be in default_config."
        for key in list(deltas.keys()):
            deltas["fixed_params" + NESTED_CONFIG_KEY_SEPARATOR + key] = deltas[key]
            del deltas[key]

    if len(deltas) > 0:
        check_duplicates(list(deltas.keys()))

    deltas_del = []
    for key in deltas.keys():
        value = deltas[key]
        if value == EMPTY_STRING:
            deltas[key] = ""
        elif value == "":
            if EMPTY_VALUE_MEANS_NO_CHANGE:
                # erase the key
                deltas_del.append(key)
                warnings.warn("WARNING: Empty value for {} will be ignored.".format(key))
            else:
                raise Exception(f"Empty value for {make_delta_column_name(key)}")
    for key in deltas_del:
        del deltas[key]

    decode_strings_in_dict(
        deltas, list_separators=[" "], list_start_symbol="[", list_end_symbol="]"
    )

    deltas[f"logging{NESTED_CONFIG_KEY_SEPARATOR}output_csv"] = make_csv_config(
        input_csv_path, row_number, spreadsheet_url, worksheet_name
    )

    deltas[f"logging{NESTED_CONFIG_KEY_SEPARATOR}server_ip"] = server_ip
    deltas[f"logging{NESTED_CONFIG_KEY_SEPARATOR}server_port"] = server_port

    deltas["run_locally"] = run_locally

    new_config = make_config_from_default_and_deltas(default_config, deltas)
    # make sure we preserve deltas though
    for delta in deltas:
        if delta == f"logging{NESTED_CONFIG_KEY_SEPARATOR}output_csv":
            continue
        new_config[DELTA_PREFIX + PREFIX_SEPARATOR + delta] = deltas[delta]

    if DELTA_AFFECTS_ONLY_FIXED_PARAMS:
        # Copy stuff from `fixed_params` to the root of the config
        for key, value in new_config["fixed_params"].items():
            new_config[key] = value

    new_config_path = os.path.join(
        exp_dir, AUTOGEN_PREFIX, make_autogenerated_config_name(input_csv_path, row_number)
    )
    os.makedirs(os.path.dirname(new_config_path), exist_ok=True)
    save_as_yaml(new_config_path, new_config)
    return new_config, new_config_path


def make_config_from_default_and_deltas(default_config, deltas):
    """Create a new configuration by applying deltas to the default configuration."""
    assert isinstance(deltas, dict)
    new_config = copy.deepcopy(default_config)
    for nested_config_key, new_value in deltas.items():
        update_dict_by_nested_key(
            new_config,
            nested_config_key.split(NESTED_CONFIG_KEY_SEPARATOR),
            new_value,
            to_create_new_elements=True,
        )
    return new_config


def replace_placeholders(csv_row, placeholder, new_value):
    """Replace placeholders in CSV row values with new values."""
    for column_name, value in csv_row.items():
        csv_row[column_name] = str(value).replace(placeholder, new_value)


def extract_from_csv_row_by_prefix(csv_row, prefix, ignore_values):
    """Extract key-value pairs from CSV row that match the given prefix."""
    prefix_len = len(prefix)
    result = {}
    for key, value in csv_row.items():
        assert (
            key is not None
        ), f"Possibly inconsistent number of delimeters. Found key={key} in csv_row={csv_row} with value={value}"
        if key == prefix:
            raise Exception(
                f'Found "{prefix}" (nothing after this prefix) ' f"in csv_row:\n{csv_row}"
            )
        if len(key) > prefix_len and prefix == key[:prefix_len] and not value in ignore_values:
            result[key[prefix_len:]] = value

    return result 